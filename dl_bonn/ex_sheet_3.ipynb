{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sequence prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import nn\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "lstm_size  = 512\n",
    "n_layers   = 3\n",
    "n_epochs   = 1000\n",
    "seq_len    = 50\n",
    "batch_size = 50\n",
    "lr = 1\n",
    "grad_norm = 5\n",
    "save_freq = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xavier(n_in, n_out):\n",
    "    init_range = 4*math.sqrt(6.0/(n_in + n_out))\n",
    "    return tf.random_uniform([n_in, n_out], -init_range, init_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('shakespeare.txt') as f:\n",
    "    words_full = [c for c in list(f.read())]\n",
    "    vocab = []\n",
    "    for c in words_full:\n",
    "        if c not in vocab:\n",
    "            vocab.append(c)\n",
    "    vocab_size = len(vocab)    \n",
    "    \n",
    "    #make batches\n",
    "    tr_x = np.array([vocab.index(el) for el in words_full], dtype='int32')\n",
    "    #target is just data shifted one element to the left\n",
    "    tr_y = np.roll(tr_x, -1)\n",
    "    sp = [i*batch_size*seq_len for i in range(1,len(words_full)//(batch_size*seq_len) + 1)]\n",
    "    n_batches = len(tr_x)//(batch_size*seq_len)\n",
    "    if(len(tr_x) % batch_size*seq_len != 0):\n",
    "        # drop last small batch\n",
    "        tr_x = np.array(np.split(tr_x, sp)[:-1]).reshape(n_batches, seq_len, batch_size)\n",
    "        tr_y = np.array(np.split(tr_y, sp)[:-1]).reshape(n_batches, seq_len, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f0e946fceb8>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "#create model\n",
    "#tf.reset_default_graph()\n",
    "\n",
    "lstm = nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "stacked_lstm = nn.rnn_cell.MultiRNNCell([lstm] * n_layers)\n",
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_len])\n",
    "targets = tf.placeholder(tf.int32, [batch_size, seq_len])\n",
    "initial_state = state = stacked_lstm.zero_state(batch_size, tf.float32)\n",
    "embedding = tf.get_variable(\"embedding\", [vocab_size, lstm_size])\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "outputs = []\n",
    "for i in range(seq_len):\n",
    "    if i > 0:\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "    cell_output, state = stacked_lstm(inputs[:,i,:], state)\n",
    "    outputs.append(cell_output)\n",
    "final_state = state\n",
    "\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, lstm_size])\n",
    "softmax_w = tf.Variable(xavier(lstm_size, vocab_size), dtype=tf.float32)\n",
    "softmax_b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "z = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(z)\n",
    "loss = tf.nn.seq2seq.sequence_loss_by_example([z], [tf.reshape(targets, [-1])], [tf.ones([batch_size * seq_len])], vocab_size)\n",
    "cost = tf.reduce_sum(loss) / batch_size / seq_len\n",
    "lr = tf.Variable(lr, trainable=False)\n",
    "optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_norm)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1, cost 228.344\n",
      " Epoch 11, cost 118.438\n",
      " Epoch 21, cost 94.516\n",
      " Epoch 31, cost 85.128\n",
      " Epoch 41, cost 78.109\n",
      " Epoch 51, cost 71.946\n",
      " Epoch 61, cost 65.599\n",
      " Epoch 71, cost 57.648\n",
      " Epoch 81, cost 47.297\n",
      " Epoch 91, cost 34.219\n",
      " Epoch 101, cost 18.339\n",
      " Epoch 111, cost 7.998\n",
      " Epoch 121, cost 6.148\n",
      " Epoch 131, cost 5.384\n",
      " Epoch 141, cost 4.971\n",
      " Epoch 151, cost 4.711\n",
      " Epoch 161, cost 4.529\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "with tf.Session() as s:\n",
    "    tf.initialize_all_variables().run()\n",
    "    state = initial_state.eval()\n",
    "    for i in range(n_epochs):\n",
    "        ep_cost = 0\n",
    "        for j in range(n_batches):\n",
    "            print(\"\\r\", 'Epoch %d, Batch %d' % (i+1, j+1), end='')\n",
    "            _, _, co, _ = s.run([output, final_state, cost, train_op], feed_dict={input_data:tr_x[j], targets:tr_y[j], initial_state: state})\n",
    "            ep_cost+=co\n",
    "        if i % save_freq == 0:\n",
    "            print(\"\\r\", 'Epoch %d, cost %.3f' % (i+1, ep_cost))\n",
    "            saver.save(s, 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x7f0dd82c98d0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "lstm = nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "stacked_lstm = nn.rnn_cell.MultiRNNCell([lstm] * n_layers)\n",
    "input_data = tf.placeholder(tf.int32, [1, 1])\n",
    "initial_state = state = stacked_lstm.zero_state(1, tf.float32)\n",
    "\n",
    "embedding = tf.get_variable(\"embedding\", [vocab_size, lstm_size])\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data)\n",
    "cell_output, state = stacked_lstm(inputs[:,0,:], state)\n",
    "outputs = [cell_output]\n",
    "final_state = state\n",
    "\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, lstm_size])\n",
    "softmax_w = tf.Variable(tf.random_normal([lstm_size, vocab_size], stddev=0.35), dtype=tf.float32)\n",
    "softmax_b = tf.Variable(tf.zeros([vocab_size]), dtype=tf.float32)\n",
    "z = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARENE:\n",
      "E: sir.\n",
      "\n",
      "Second Sartert:\n",
      "The morrach, and us much domner wond and so tear you sere\n",
      "As it he it to came that them.\n",
      "\n",
      "BLOTRSSA:\n",
      "I thou not werke in, lort, preatent, for conselt thou wint\n",
      "cill thou hear.\n",
      "\n",
      "SAUK lot Antomita's his distrect comasn in dink come that a fitler.\n",
      "\n",
      "BUKE HoRY I I make that him: had it it not\n",
      "I me her that with came again,\n",
      "To coneer thou shall hoad them our ready, and teat is hipos,\n",
      "But Too his heard shemind dothere he was ding ears there torre\n",
      "Co omeerrible stas gan a called out the wort they souldss lederone deador\n",
      "The know his is 'as inder, to mast my affaces's see,\n",
      "That in suve seven to tather my throught\n",
      "Hen leave I longen stach what the hands to fat in lifer,\n",
      "When shall man his preat in the partifes of a my riftaf; Bthald lord on Purg:\n",
      "I say all the poon the forth op the our palre,\n",
      "And fas we shoull for and him to peen usery to your come andre\n",
      "And sirch and blood ham strein; with be mendition, bry thinoger\n",
      "Shall Lood the speaken of fece do a rud mistles\n",
      "Th\n"
     ]
    }
   ],
   "source": [
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    state = initial_state.eval()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state('model')\n",
    "    saver.restore(sess, 'model')\n",
    "\n",
    "    inpt = 'LEAR'\n",
    "    sample_size = 1000\n",
    "    for c in inpt[:-1]:\n",
    "        x = np.zeros((1,1), dtype='int32')\n",
    "        x [0,0] = vocab.index(c)\n",
    "        state = sess.run(final_state, feed_dict={input_data:x, initial_state:state})\n",
    "    \n",
    "    res = inpt\n",
    "    char = inpt[-1]\n",
    "\n",
    "    for i in range(sample_size):\n",
    "        x = np.zeros((1,1), dtype='int32')\n",
    "        x[0, 0] = vocab.index(char)\n",
    "        probabilities, state = sess.run([probs, final_state], feed_dict={input_data:x, initial_state:state})\n",
    "        p = probabilities[0]\n",
    "        sample = np.argmax(p)\n",
    "        pred = vocab[sample]\n",
    "        res += pred\n",
    "        char = pred\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
