{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#http://deeplearning.net/tutorial/logreg.html\n",
    "import pickle,gzip,os,timeit\n",
    "import numpy,theano,theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        self.W = theano.shared(value=numpy.zeros((n_in, n_out),dtype=theano.config.floatX),name='W',borrow=True)\n",
    "        self.b = theano.shared(value=numpy.zeros((n_out,),dtype=theano.config.floatX),name='b',borrow=True)\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input, self.W) + self.b)\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        self.params = [self.W, self.b]\n",
    "        self.input = input # keep track of model input\n",
    "\n",
    "    def negative_log_likelihood(self, y):\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "\n",
    "    def errors(self, y):\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError('y should have the same shape as y_pred',('y', y.type, 'y_pred', self.y_pred.type))\n",
    "        if y.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.y_pred, y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "        import urllib\n",
    "        origin = ('http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz')\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "    print('... loading data')\n",
    "\n",
    "    f = gzip.open(dataset, 'rb')\n",
    "    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "\n",
    "    def shared_dataset(data_xy, borrow=True):\n",
    "        data_x, data_y = data_xy\n",
    "        shared_x = theano.shared(numpy.asarray(data_x,dtype=theano.config.floatX),borrow=borrow)\n",
    "        shared_y = theano.shared(numpy.asarray(data_y,dtype=theano.config.floatX),borrow=borrow)\n",
    "        return shared_x, T.cast(shared_y, 'int32')\n",
    "\n",
    "    test_set_x, test_set_y = shared_dataset(test_set)\n",
    "    valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "    train_set_x, train_set_y = shared_dataset(train_set)\n",
    "\n",
    "    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),(test_set_x, test_set_y)]\n",
    "    return rval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sgd_optimization_mnist(learning_rate=0.13, n_epochs=1000,dataset='mnist.pkl.gz',batch_size=600):\n",
    "    \n",
    "    datasets = load_data(dataset)\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_valid_batches = valid_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_test_batches  = test_set_x.get_value(borrow=True).shape[0]  // batch_size\n",
    "\n",
    "    print('... building the model')\n",
    "\n",
    "    index = T.lscalar()  # index to a [mini]batch\n",
    "    x = T.matrix('x')  # data\n",
    "    y = T.ivector('y') # labels\n",
    "    classifier = LogisticRegression(input=x, n_in=28 * 28, n_out=10) # Each MNIST image has size 28*28\n",
    "    cost = classifier.negative_log_likelihood(y)\n",
    "\n",
    "    test_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    validate_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=classifier.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "    g_b = T.grad(cost=cost, wrt=classifier.b)\n",
    "    updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "    train_model = theano.function(\n",
    "        inputs=[index],\n",
    "        outputs=cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print('... training the model')\n",
    "    patience = 5000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is found\n",
    "    improvement_threshold = 0.995  # a relative improvement of this much is considered significant\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    epoch = 0\n",
    "    quit_flag = False\n",
    "    while (epoch < n_epochs) and not quit_flag:\n",
    "        epoch += 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            minibatch_avg_cost = train_model(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_loss = numpy.mean([validate_model(i) for i in range(n_valid_batches)])\n",
    "                if validation_loss < best_validation_loss:\n",
    "                    #improve patience if loss improvement is good enough\n",
    "                    if validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                    best_validation_loss = validation_loss\n",
    "                    test_losses = [test_model(i) for i in range(n_test_batches)]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('    epoch %i, minibatch %i/%i, test error of best model %f %%'\n",
    "                        ) %(epoch,minibatch_index + 1,n_train_batches,test_score * 100.))\n",
    "                    \n",
    "                    with open('best_model.pkl', 'wb') as f:\n",
    "                        pickle.dump(classifier, f) # save the best model\n",
    "\n",
    "            if patience <= iter:\n",
    "                quit_flag = True\n",
    "                break\n",
    "    processing_time = timeit.default_timer() - start_time\n",
    "    print('Optimisation completed')\n",
    "    print('Best validation score of %f %%, test performance %f %%' % (best_validation_loss * 100., test_score * 100.))\n",
    "    print('%d epochs ran for %fs with %f epochs/sec' % (epoch, processing_time, 1. * epoch / processing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    classifier = pickle.load(open('best_model.pkl', 'rb'))\n",
    "    predict_model = theano.function(inputs=[classifier.input], outputs=classifier.y_pred)\n",
    "\n",
    "    dataset='mnist.pkl.gz'\n",
    "    datasets = load_data(dataset)\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "    test_set_x = test_set_x.get_value()\n",
    "\n",
    "    predicted_values = predict_model(test_set_x[:100][:100])\n",
    "    print(\"Predicted values for the first 10 examples in test set:\")\n",
    "    print(predicted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... building the model\n",
      "... training the model\n",
      "    epoch 1, minibatch 83/83, test error of best model 12.375000 %\n",
      "    epoch 2, minibatch 83/83, test error of best model 10.958333 %\n",
      "    epoch 3, minibatch 83/83, test error of best model 10.312500 %\n",
      "    epoch 4, minibatch 83/83, test error of best model 9.833333 %\n",
      "    epoch 5, minibatch 83/83, test error of best model 9.479167 %\n",
      "    epoch 6, minibatch 83/83, test error of best model 9.291667 %\n",
      "    epoch 7, minibatch 83/83, test error of best model 9.000000 %\n",
      "    epoch 8, minibatch 83/83, test error of best model 8.958333 %\n",
      "    epoch 9, minibatch 83/83, test error of best model 8.812500 %\n",
      "    epoch 10, minibatch 83/83, test error of best model 8.666667 %\n",
      "    epoch 11, minibatch 83/83, test error of best model 8.520833 %\n",
      "    epoch 12, minibatch 83/83, test error of best model 8.416667 %\n",
      "    epoch 13, minibatch 83/83, test error of best model 8.291667 %\n",
      "    epoch 14, minibatch 83/83, test error of best model 8.281250 %\n",
      "    epoch 15, minibatch 83/83, test error of best model 8.270833 %\n",
      "    epoch 16, minibatch 83/83, test error of best model 8.239583 %\n",
      "    epoch 17, minibatch 83/83, test error of best model 8.177083 %\n",
      "    epoch 18, minibatch 83/83, test error of best model 8.062500 %\n",
      "    epoch 21, minibatch 83/83, test error of best model 7.947917 %\n",
      "    epoch 22, minibatch 83/83, test error of best model 7.927083 %\n",
      "    epoch 23, minibatch 83/83, test error of best model 7.958333 %\n",
      "    epoch 24, minibatch 83/83, test error of best model 7.947917 %\n",
      "    epoch 25, minibatch 83/83, test error of best model 7.947917 %\n",
      "    epoch 28, minibatch 83/83, test error of best model 7.843750 %\n",
      "    epoch 30, minibatch 83/83, test error of best model 7.843750 %\n",
      "    epoch 31, minibatch 83/83, test error of best model 7.833333 %\n",
      "    epoch 32, minibatch 83/83, test error of best model 7.812500 %\n",
      "    epoch 33, minibatch 83/83, test error of best model 7.739583 %\n",
      "    epoch 34, minibatch 83/83, test error of best model 7.729167 %\n",
      "    epoch 36, minibatch 83/83, test error of best model 7.697917 %\n",
      "    epoch 37, minibatch 83/83, test error of best model 7.635417 %\n",
      "    epoch 41, minibatch 83/83, test error of best model 7.625000 %\n",
      "    epoch 42, minibatch 83/83, test error of best model 7.614583 %\n",
      "    epoch 43, minibatch 83/83, test error of best model 7.593750 %\n",
      "    epoch 44, minibatch 83/83, test error of best model 7.593750 %\n",
      "    epoch 48, minibatch 83/83, test error of best model 7.583333 %\n",
      "    epoch 49, minibatch 83/83, test error of best model 7.572917 %\n",
      "    epoch 52, minibatch 83/83, test error of best model 7.541667 %\n",
      "    epoch 54, minibatch 83/83, test error of best model 7.520833 %\n",
      "    epoch 57, minibatch 83/83, test error of best model 7.489583 %\n",
      "    epoch 58, minibatch 83/83, test error of best model 7.458333 %\n",
      "    epoch 59, minibatch 83/83, test error of best model 7.468750 %\n",
      "    epoch 62, minibatch 83/83, test error of best model 7.520833 %\n",
      "    epoch 63, minibatch 83/83, test error of best model 7.510417 %\n",
      "    epoch 66, minibatch 83/83, test error of best model 7.520833 %\n",
      "    epoch 68, minibatch 83/83, test error of best model 7.520833 %\n",
      "    epoch 70, minibatch 83/83, test error of best model 7.500000 %\n",
      "    epoch 73, minibatch 83/83, test error of best model 7.489583 %\n",
      "Optimisation completed\n",
      "Best validation score of 7.500000 %, test performance 7.489583 %\n",
      "74 epochs ran for 12.545017s with 5.898756 epochs/sec\n"
     ]
    }
   ],
   "source": [
    "sgd_optimization_mnist()\n",
    "#predict()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
